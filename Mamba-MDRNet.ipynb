{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMu1SD6y9M8NN2A4ut8eENA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data preprocessing"],"metadata":{"id":"XJ6WuGL8sCQV"}},{"cell_type":"markdown","source":["## Image data processing"],"metadata":{"id":"M2tGIETIsCut"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8WJD0USsBmh"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class ImageBlock(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(44944, 120)\n","        self.fc2 = nn.Linear(120, 100)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return x\n","\n","\n","img_block = ImageBlock().to(device)\n","print(img_block)\n","img_block(b_img.to(device)).shape"]},{"cell_type":"markdown","source":["## Text data processing"],"metadata":{"id":"0WtbFoDPsPbt"}},{"cell_type":"code","source":["class textCNN(nn.Module):\n","    def __init__(self,):\n","        super().__init__()\n","        emb_dim = 100\n","        n_class = 2\n","        kernels=[3,4,5]\n","        kernel_number=[150,150,150]\n","        self.embd = nn.Embedding(voc_size, emb_dim)\n","        self.convs = nn.ModuleList([nn.Conv1d(max_len, number, size,padding=size) for (size,number) in zip(kernels,kernel_number)])\n","        self.dropout=nn.Dropout(0.1)\n","        self.out = nn.Linear(sum(kernel_number), 100)\n","\n","    def forward(self, x):\n","        x = self.embd(x)\n","\n","        x = [F.relu(conv(x)) for conv in self.convs]\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n","        x = torch.cat(x, 1)\n","        x = self.dropout(x)\n","        x = self.out(x)\n","        return x\n","text_cnn = textCNN().to(device)\n","\n","\n","text_cnn(b_text.to(device)).shape"],"metadata":{"id":"FjredsYqsuc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Probabilistic data processing"],"metadata":{"id":"-_sojhHWsznN"}},{"cell_type":"code","source":["class vecBlock(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(11, 120)\n","        self.fc2 = nn.Linear(120, 100)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return x\n","\n","\n","vec_block = vecBlock().to(device)\n","print(vec_block)\n","vec_block(b_vec.to(device)).shape"],"metadata":{"id":"3yYXLmj_sy9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define model\n","class bigModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.img_block = ImageBlock()\n","        self.text_block = textCNN()\n","        self.vec_block = vecBlock()\n","        self.relu = nn.ReLU()\n","        self.lin1 = nn.Linear(300, 50)\n","        self.lin2 = nn.Linear(50, n_class)\n","\n","\n","    def forward(self, img,text,vec):\n","        x1 = self.img_block(img)\n","        x2 = self.text_block(text)\n","        x3 = self.vec_block(vec)\n","\n","        x = torch.cat([x1,x2,x3],dim=1)\n","        x = self.flatten(x)\n","        x = self.lin1(x)\n","        x = self.lin2(x)\n","        # x =\n","        return x\n","big_model = bigModel().to(device)\n","big_model(b_img.to(device),b_text.to(device),b_vec.to(device))"],"metadata":{"id":"ckzin6Prs9DV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model building"],"metadata":{"id":"PGbj_xkwtEIr"}},{"cell_type":"code","source":["# Define model\n","class bigModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.img_block = ImageBlock()\n","        self.text_block = textCNN()\n","        self.vec_block = vecBlock()\n","        self.relu = nn.ReLU()\n","        self.lin1 = nn.Linear(300, 50)\n","        self.lin2 = nn.Linear(50, n_class)\n","\n","\n","    def forward(self, img,text,vec):\n","        x1 = self.img_block(img)\n","        x2 = self.text_block(text)\n","        x3 = self.vec_block(vec)\n","\n","        x = torch.cat([x1,x2,x3],dim=1)\n","        x = self.flatten(x)\n","        x = self.lin1(x)\n","        x = self.lin2(x)\n","        # x =\n","        return x\n","big_model = bigModel().to(device)\n","big_model(b_img.to(device),b_text.to(device),b_vec.to(device))\n","\n","loss_fn = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(big_model.parameters(), lr=2e-5)"],"metadata":{"id":"Mo8FpjxTtN5E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"Ow3fnoyStx1N"}},{"cell_type":"code","source":["epochs = 50\n","train_loss_list = []\n","train_acc_list = []\n","test_loss_list = []\n","test_acc_list = []\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(train_dl, big_model, loss_fn, optimizer)\n","\n","    train_loss, train_correct = test(train_dl, big_model, loss_fn)\n","    test_loss, test_correct = test(test_dl, big_model, loss_fn)\n","    train_loss_list.append(train_loss)\n","    train_acc_list.append(train_correct)\n","    test_loss_list.append(test_loss)\n","    test_acc_list.append(test_correct)\n","print(\"Done!\")"],"metadata":{"id":"mgvk0D5DtuwN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical indicator analysis"],"metadata":{"id":"iYKw95TTuG99"}},{"cell_type":"markdown","source":["## Classification report"],"metadata":{"id":"YXxAZJcyuOH-"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(classification_report(label_array, predict_array,digits=3))"],"metadata":{"id":"La0Y1Ca3t5K8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confusion matrix"],"metadata":{"id":"j0AicOTSuTRF"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","cnf_matrix = confusion_matrix(label_array, predict_array)\n","cnf_matrix"],"metadata":{"id":"YSt8f4s7uFzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["More detailed code will be provided after the paper is officially published."],"metadata":{"id":"sF5cFX5WubB8"}}]}